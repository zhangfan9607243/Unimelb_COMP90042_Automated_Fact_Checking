# Unimelb COMP90042 Automatic Fact Checking - Climate Change Claims

## Acknowledgement
I would like to extend my sincere gratitude to the Unimelb COMP90042 2023S1 teaching team for providing me with the opportunity to work on this project, as well as for their guidance and feedback on my work.

## Project Introduction

This project focuses on building a automatic fact-checking system for climate science claims.

The goal of this project is to create an automated fact-checking system that can:

1. Task 1: Retrieve relevant evidence from a knowledge base based on the input claim.
2. Task 2: Classify the claim as one of the following:
   - `SUPPORTS`: Evidence supports the claim.
   - `REFUTES`: Evidence refutes the claim.
   - `NOT_ENOUGH_INFO`: Evidence is insufficient to determine the truthfulness.
   - `DISPUTED`: Evidence is inconclusive or contradictory.

The final system's performance is evaluated using **Codalab**, where both evidence retrieval and claim classification are measured.

## Files Introduction

Please ensure you set up the following additional directory structure before running the project:

```
automatic_fact_checking/
│ 
├── data/
│   ├── data_original/
│   ├── data_task0/
│   ├── data_task1/
│   └── data_task2/
│
├── task0/
│
├── task1/
│
└── task2/
```

The following is detailed explanation of folders and files:

- `data/`: The folder for original data, processed data, and the output data

  - `data_original/`: The folder for original data, which contains the training, validation, and testing datasets. The data can be downloaded from the [link](https://pan.baidu.com/s/1w0JkQVTIPZHkocowz8r2EA?pwd=xvjb). Please unzip the downloaded file and put all the contents into this folder.
  - `data_task0/`: The folder for data processed for task 0, which is the data filtering step. It contains the filtered evidence for each claim. The data will be generated after running `task0/task0.ipynb`.
  - `data_task1/`: The folder for output of task 1, which is the evidence retrieval step. The data will be generated after running `task1/task1.ipynb`.
  - `data_task2/`: The folder for output of task 2, which is the claim classification step. The data will be generated after running `task2/task2.ipynb`.

- `task0/`: The folder for code of task 0, which is the data filtering step.

- `task1/`: The folder for code of task 1, which is the evidence retrieval step.

- `task2/`: The folder for code of task 2, which is the claim classification step.

## Task 0: Data Filtering

Before implementing the evidence retrieval and claim classification models, we first need to filter the evidences for each claim, in order to reduce the number of evidences that need to be compared with each claim, which can significantly improve the efficiency of the following two tasks.

The logic of this task is simple: if an evidence is related to a claim, then they should have at least one common unigram none or proper noun. Therefore, we can filter the evidences for each claim by checking whether they have at least one common unigram none or proper noun.

In prctice, we set the threshold of the number of common unigram none or proper nouns to 2, which means that if an evidence has at least 2 common unigram none or proper nouns with a claim, then we consider this evidence as a relevant evidence for this claim. This can not only help us retain relevant evidences for each claim, but also keep the number of evidences for each claim at a reasonable level.

## Task 1: Evidence Retrieval

After filtering the evidences for each claim, we train a distilled RoBERTa model to retrieve the relevant evidences for each claim from the filtered evidences.

The negative samples are generated by randomly selecting evidences that are not relevant to the claim from the filtered evidences, while the positive samples are the relevant evidences identified in the original data.

After training the model, we use it to predict the relevance score of each evidence for each claim, and select the top 6 evidences with the highest relevance scores as the retrieved evidences for each claim. The predicted result is going to be stored as `/data/data_task1/data_task1.json`.

## Task 2: Claim Classification

In this task, we also train a distilled RoBERTa model, to identify the label of a claim, using the combined text of claim and its relevant evidences. 

We transform the label of each claim into a 2-dimensional vector, like `[-1, 1]`. The first element means the support score, ranging from -1 to 1, while the second element means the relevant score, ranging from 0 to 1. 

- If the original label is `NOT_ENOUGH_INFO`, then the transformed label is `[0, 0]`. The second element being 0 means that the claim is not relevant to any evidence, while the first element being 0 means nothing in this case, since the claim is not relevant to any evidence.
- If the original label is `SUPPORTS`, then the transformed label is `[1, 1]`. The second element being 1 means that the claim is relevant to at least one evidence, while the first element being 1 means that the claim is supported by at least one evidence. 
- If the original label is `REFUTES`, then the transformed label is `[-1, 1]`. The second element being 1 means that the claim is relevant to at least one evidence, while the first element being -1 means that the claim is refuted by at least one evidence.
- If the original label is `DISPUTED`, then the transformed label is `[0, 1]`. The second element being 1 means that the claim is relevant to at least one evidence, while the first element being 0 means that the claim is neither supported nor refuted by any evidence.

This logic is also applied to the predicted label of each claim:

- First we assess the second element of the predicted label: 

  - If the second element is less than 0.5, then we consider this claim as `NOT_ENOUGH_INFO`
  - If the second element is greater than or equal to 0.5, then we further assess the first element of the predicted label

- When assessing the first element of the predicted label, we have three cases:

  - If the first element is greater than 0.5, then we consider this claim as `SUPPORTS`
  - If the first element is less than -0.5, then we consider this claim as `REFUTES`
  - Otherwise, we consider this claim as `DISPUTED`

The predicted result is going to be stored as `/data/data_task2/data_task2.json`.

## Final Result

The results of task 1 and task 2 are as follows:

|                | Task 1: Accuracy | Task 2: Accuracy | Overall: Mean of Accuracy |
|--------------- | ---------------- | ---------------- | --------------------------|
| Training Set   | 0.9933 | 0.9741 | 0.9837 |
| Validation Set | 0.9126 | 0.7468 | 0.8297 |
| Testing Set    | 0.8027 | 0.6837 | 0.7432 |
