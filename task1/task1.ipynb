{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80b96cc3",
   "metadata": {},
   "source": [
    "# Task 1 - Retrieve Evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30947a9d",
   "metadata": {},
   "source": [
    "## 1. Prepare Data for Retrieval Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4294e56",
   "metadata": {},
   "source": [
    "## 1.1 Read Data from Task 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f192caba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizer, RobertaModel, DistilBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43a5b742",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tran_task0 = pd.read_json(\"../data/data_task0/df_tran_matched.json\", orient='index')\n",
    "data_vald_task0 = pd.read_json(\"../data/data_task0/df_vald_matched.json\", orient='index')\n",
    "data_test_task0 = pd.read_json(\"../data/data_task0/df_test_matched.json\", orient='index')\n",
    "data_evdn_task0 = pd.read_json(\"../data/data_task0/df_evdn.json\", orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1009d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training data overview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>claim</th>\n",
       "      <th>label</th>\n",
       "      <th>evidences</th>\n",
       "      <th>evidences_matched</th>\n",
       "      <th>evidences_mismatched</th>\n",
       "      <th>evidences_missed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1937</td>\n",
       "      <td>Not only is there no scientific evidence that ...</td>\n",
       "      <td>DISPUTED</td>\n",
       "      <td>[442946, 1194317, 12171]</td>\n",
       "      <td>[215, 315, 441, 526, 783, 968, 1018, 1135, 119...</td>\n",
       "      <td>[491521, 917507, 393220, 32773, 819207, 475144...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>126</td>\n",
       "      <td>El Niño drove record highs in global temperatu...</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>[338219, 1127398]</td>\n",
       "      <td>[301, 441, 1018, 1135, 1236, 1579, 1758, 1823,...</td>\n",
       "      <td>[491521, 917507, 32773, 327686, 1130503, 47514...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2510</td>\n",
       "      <td>In 1946, PDO switched to a cool phase.</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[530063, 984887]</td>\n",
       "      <td>[74, 215, 226, 301, 357, 411, 552, 680, 694, 8...</td>\n",
       "      <td>[491521, 1146885, 655369, 688143, 1114128, 229...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                              claim     label  \\\n",
       "0  1937  Not only is there no scientific evidence that ...  DISPUTED   \n",
       "1   126  El Niño drove record highs in global temperatu...   REFUTES   \n",
       "2  2510             In 1946, PDO switched to a cool phase.  SUPPORTS   \n",
       "\n",
       "                  evidences  \\\n",
       "0  [442946, 1194317, 12171]   \n",
       "1         [338219, 1127398]   \n",
       "2          [530063, 984887]   \n",
       "\n",
       "                                   evidences_matched  \\\n",
       "0  [215, 315, 441, 526, 783, 968, 1018, 1135, 119...   \n",
       "1  [301, 441, 1018, 1135, 1236, 1579, 1758, 1823,...   \n",
       "2  [74, 215, 226, 301, 357, 411, 552, 680, 694, 8...   \n",
       "\n",
       "                                evidences_mismatched evidences_missed  \n",
       "0  [491521, 917507, 393220, 32773, 819207, 475144...               []  \n",
       "1  [491521, 917507, 32773, 327686, 1130503, 47514...               []  \n",
       "2  [491521, 1146885, 655369, 688143, 1114128, 229...               []  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original validation data overview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>claim</th>\n",
       "      <th>label</th>\n",
       "      <th>evidences</th>\n",
       "      <th>evidences_matched</th>\n",
       "      <th>evidences_mismatched</th>\n",
       "      <th>evidences_missed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>752</td>\n",
       "      <td>[South Australia] has the most expensive elect...</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[67732, 572512]</td>\n",
       "      <td>[16, 37, 89, 514, 608, 1001, 1291, 1457, 1577,...</td>\n",
       "      <td>[1114115, 933896, 786443, 917516, 16, 262162, ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>375</td>\n",
       "      <td>when 3 per cent of total annual global emissio...</td>\n",
       "      <td>NOT_ENOUGH_INFO</td>\n",
       "      <td>[996421, 1080858, 208053, 699212, 832334]</td>\n",
       "      <td>[16, 37, 410, 414, 441, 465, 514, 608, 783, 80...</td>\n",
       "      <td>[524291, 917507, 720899, 1114115, 786443, 16, ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1266</td>\n",
       "      <td>This means that the world is now 1C warmer tha...</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>[889933, 694262]</td>\n",
       "      <td>[975, 1061, 1830, 2487, 4035, 4321, 5354, 6025...</td>\n",
       "      <td>[856065, 520194, 1036290, 1114114, 692229, 565...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                              claim            label  \\\n",
       "0   752  [South Australia] has the most expensive elect...         SUPPORTS   \n",
       "1   375  when 3 per cent of total annual global emissio...  NOT_ENOUGH_INFO   \n",
       "2  1266  This means that the world is now 1C warmer tha...         SUPPORTS   \n",
       "\n",
       "                                   evidences  \\\n",
       "0                            [67732, 572512]   \n",
       "1  [996421, 1080858, 208053, 699212, 832334]   \n",
       "2                           [889933, 694262]   \n",
       "\n",
       "                                   evidences_matched  \\\n",
       "0  [16, 37, 89, 514, 608, 1001, 1291, 1457, 1577,...   \n",
       "1  [16, 37, 410, 414, 441, 465, 514, 608, 783, 80...   \n",
       "2  [975, 1061, 1830, 2487, 4035, 4321, 5354, 6025...   \n",
       "\n",
       "                                evidences_mismatched evidences_missed  \n",
       "0  [1114115, 933896, 786443, 917516, 16, 262162, ...               []  \n",
       "1  [524291, 917507, 720899, 1114115, 786443, 16, ...               []  \n",
       "2  [856065, 520194, 1036290, 1114114, 692229, 565...               []  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original test data overview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>claim</th>\n",
       "      <th>evidences_matched</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2967</td>\n",
       "      <td>The contribution of waste heat to the global c...</td>\n",
       "      <td>[1460, 8304, 11688, 13436, 19067, 22107, 26773...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>979</td>\n",
       "      <td>“Warm weather worsened the most recent five-ye...</td>\n",
       "      <td>[10368, 13434, 16837, 19622, 21546, 27162, 289...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1609</td>\n",
       "      <td>Greenland has only lost a tiny fraction of its...</td>\n",
       "      <td>[5928, 10210, 19306, 29169, 32006, 44485, 4475...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                              claim  \\\n",
       "0  2967  The contribution of waste heat to the global c...   \n",
       "1   979  “Warm weather worsened the most recent five-ye...   \n",
       "2  1609  Greenland has only lost a tiny fraction of its...   \n",
       "\n",
       "                                   evidences_matched  \n",
       "0  [1460, 8304, 11688, 13436, 19067, 22107, 26773...  \n",
       "1  [10368, 13434, 16837, 19622, 21546, 27162, 289...  \n",
       "2  [5928, 10210, 19306, 29169, 32006, 44485, 4475...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original evidence data overview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>evidence_id</th>\n",
       "      <th>evidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>John Bennet Lawes, English entrepreneur and ag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Lindberg began his professional career at the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>``Boston (Ladies of Cambridge)'' by Vampire We...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   evidence_id                                           evidence\n",
       "0            0  John Bennet Lawes, English entrepreneur and ag...\n",
       "1            1  Lindberg began his professional career at the ...\n",
       "2            2  ``Boston (Ladies of Cambridge)'' by Vampire We..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Original training data overview:\")\n",
    "display(data_tran_task0.head(3))\n",
    "print(\"Original validation data overview:\")\n",
    "display(data_vald_task0.head(3))\n",
    "print(\"Original test data overview:\")\n",
    "display(data_test_task0.head(3))\n",
    "print(\"Original evidence data overview:\")\n",
    "display(data_evdn_task0.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05c1cb1",
   "metadata": {},
   "source": [
    "## 1.2 Positive Sampling and Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a5102fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_pos_neg(data_origin, data_evdn, is_test=False):\n",
    "    samples = []\n",
    "    for idx, row in data_origin.iterrows():\n",
    "        claim_id = row[\"id\"]\n",
    "        text = row['claim']\n",
    "        # Positive samples\n",
    "        evidences = row['evidences'] if not is_test else row['evidences_matched']\n",
    "        for ev_id in evidences:\n",
    "            if ev_id in data_evdn.index:\n",
    "                ev_text = data_evdn.loc[ev_id, 'evidence']\n",
    "                if is_test:\n",
    "                    samples.append({'claim_id': claim_id, 'text': text, 'evidence_id': ev_id, 'evidence': ev_text})\n",
    "                else:\n",
    "                    samples.append({'claim_id': claim_id, 'text': text, 'evidence_id': ev_id, 'evidence': ev_text, 'label': 1})\n",
    "        # Negative samples\n",
    "        if not is_test:\n",
    "            evidences_mismatched = row['evidences_mismatched']\n",
    "            # For train/val, sample 5-10 negatives\n",
    "            valid_ev_ids = [ev_id for ev_id in evidences_mismatched if ev_id in data_evdn.index]\n",
    "            num_to_sample = min(len(valid_ev_ids), random.randint(5, 10))\n",
    "            sampled_ev_ids = random.sample(valid_ev_ids, num_to_sample)\n",
    "            for ev_id in sampled_ev_ids:\n",
    "                ev_text = data_evdn.loc[ev_id, 'evidence']\n",
    "                samples.append({'claim_id': claim_id, 'text': text, 'evidence_id': ev_id, 'evidence': ev_text, 'label': 0})\n",
    "    return pd.DataFrame(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17c3252e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create samples\n",
    "df_tran = sampling_pos_neg(data_tran_task0, data_evdn_task0)\n",
    "df_tran_pos_count = df_tran[df_tran['label'] == 1].shape[0]\n",
    "df_tran_neg_count = df_tran[df_tran['label'] == 0].shape[0]\n",
    "\n",
    "df_vald = sampling_pos_neg(data_vald_task0, data_evdn_task0)\n",
    "df_vald_pos_count = df_vald[df_vald['label'] == 1].shape[0]\n",
    "df_vald_neg_count = df_vald[df_vald['label'] == 0].shape[0]\n",
    "\n",
    "df_test = sampling_pos_neg(data_test_task0, data_evdn_task0, is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25e2d101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training positive samples count: 4122\n",
      "Training negative samples count: 9177\n",
      "Training samples overview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim_id</th>\n",
       "      <th>text</th>\n",
       "      <th>evidence_id</th>\n",
       "      <th>evidence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1937</td>\n",
       "      <td>Not only is there no scientific evidence that ...</td>\n",
       "      <td>442946</td>\n",
       "      <td>At very high concentrations (100 times atmosph...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1937</td>\n",
       "      <td>Not only is there no scientific evidence that ...</td>\n",
       "      <td>1194317</td>\n",
       "      <td>Plants can grow as much as 50 percent faster i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1937</td>\n",
       "      <td>Not only is there no scientific evidence that ...</td>\n",
       "      <td>12171</td>\n",
       "      <td>Higher carbon dioxide concentrations will favo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1937</td>\n",
       "      <td>Not only is there no scientific evidence that ...</td>\n",
       "      <td>887285</td>\n",
       "      <td>While the principal greenhouse gas emission fr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1937</td>\n",
       "      <td>Not only is there no scientific evidence that ...</td>\n",
       "      <td>683138</td>\n",
       "      <td>(BBC) 4 April A new, detailed record of past c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1937</td>\n",
       "      <td>Not only is there no scientific evidence that ...</td>\n",
       "      <td>272774</td>\n",
       "      <td>During times of intense precipitation (such as...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1937</td>\n",
       "      <td>Not only is there no scientific evidence that ...</td>\n",
       "      <td>900415</td>\n",
       "      <td>McKibben began his freelance writing career at...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1937</td>\n",
       "      <td>Not only is there no scientific evidence that ...</td>\n",
       "      <td>339682</td>\n",
       "      <td>Hamilton's general view about climate change i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1937</td>\n",
       "      <td>Not only is there no scientific evidence that ...</td>\n",
       "      <td>318323</td>\n",
       "      <td>Certain agricultural demands may increase more...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1937</td>\n",
       "      <td>Not only is there no scientific evidence that ...</td>\n",
       "      <td>438019</td>\n",
       "      <td>The process involves reacting carbon dioxide w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   claim_id                                               text  evidence_id  \\\n",
       "0      1937  Not only is there no scientific evidence that ...       442946   \n",
       "1      1937  Not only is there no scientific evidence that ...      1194317   \n",
       "2      1937  Not only is there no scientific evidence that ...        12171   \n",
       "3      1937  Not only is there no scientific evidence that ...       887285   \n",
       "4      1937  Not only is there no scientific evidence that ...       683138   \n",
       "5      1937  Not only is there no scientific evidence that ...       272774   \n",
       "6      1937  Not only is there no scientific evidence that ...       900415   \n",
       "7      1937  Not only is there no scientific evidence that ...       339682   \n",
       "8      1937  Not only is there no scientific evidence that ...       318323   \n",
       "9      1937  Not only is there no scientific evidence that ...       438019   \n",
       "\n",
       "                                            evidence  label  \n",
       "0  At very high concentrations (100 times atmosph...      1  \n",
       "1  Plants can grow as much as 50 percent faster i...      1  \n",
       "2  Higher carbon dioxide concentrations will favo...      1  \n",
       "3  While the principal greenhouse gas emission fr...      0  \n",
       "4  (BBC) 4 April A new, detailed record of past c...      0  \n",
       "5  During times of intense precipitation (such as...      0  \n",
       "6  McKibben began his freelance writing career at...      0  \n",
       "7  Hamilton's general view about climate change i...      0  \n",
       "8  Certain agricultural demands may increase more...      0  \n",
       "9  The process involves reacting carbon dioxide w...      0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation positive samples count: 491\n",
      "Validation negative samples count: 1163\n",
      "Validation samples overview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim_id</th>\n",
       "      <th>text</th>\n",
       "      <th>evidence_id</th>\n",
       "      <th>evidence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>752</td>\n",
       "      <td>[South Australia] has the most expensive elect...</td>\n",
       "      <td>67732</td>\n",
       "      <td>[citation needed] South Australia has the high...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>752</td>\n",
       "      <td>[South Australia] has the most expensive elect...</td>\n",
       "      <td>572512</td>\n",
       "      <td>\"South Australia has the highest power prices ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>752</td>\n",
       "      <td>[South Australia] has the most expensive elect...</td>\n",
       "      <td>966583</td>\n",
       "      <td>Through the program, the organizations created...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>752</td>\n",
       "      <td>[South Australia] has the most expensive elect...</td>\n",
       "      <td>235036</td>\n",
       "      <td>This power is normally generated at power plan...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>752</td>\n",
       "      <td>[South Australia] has the most expensive elect...</td>\n",
       "      <td>482095</td>\n",
       "      <td>Though the country 's supply of electricity ne...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>752</td>\n",
       "      <td>[South Australia] has the most expensive elect...</td>\n",
       "      <td>747324</td>\n",
       "      <td>Although Mew Mew Power has not been released t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>752</td>\n",
       "      <td>[South Australia] has the most expensive elect...</td>\n",
       "      <td>1090285</td>\n",
       "      <td>Visitors from the USA, Brazil, Japan, China, A...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>752</td>\n",
       "      <td>[South Australia] has the most expensive elect...</td>\n",
       "      <td>230559</td>\n",
       "      <td>The first checkpoint was at Berri, South Austr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>752</td>\n",
       "      <td>[South Australia] has the most expensive elect...</td>\n",
       "      <td>783374</td>\n",
       "      <td>A study done by Greenpeace International, the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>752</td>\n",
       "      <td>[South Australia] has the most expensive elect...</td>\n",
       "      <td>710661</td>\n",
       "      <td>Due to Lesotho's economic and geographical rel...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>752</td>\n",
       "      <td>[South Australia] has the most expensive elect...</td>\n",
       "      <td>1202442</td>\n",
       "      <td>It is found in Australia, where it has been re...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>752</td>\n",
       "      <td>[South Australia] has the most expensive elect...</td>\n",
       "      <td>1182965</td>\n",
       "      <td>The institute’s mission is to create a sociall...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    claim_id                                               text  evidence_id  \\\n",
       "0        752  [South Australia] has the most expensive elect...        67732   \n",
       "1        752  [South Australia] has the most expensive elect...       572512   \n",
       "2        752  [South Australia] has the most expensive elect...       966583   \n",
       "3        752  [South Australia] has the most expensive elect...       235036   \n",
       "4        752  [South Australia] has the most expensive elect...       482095   \n",
       "5        752  [South Australia] has the most expensive elect...       747324   \n",
       "6        752  [South Australia] has the most expensive elect...      1090285   \n",
       "7        752  [South Australia] has the most expensive elect...       230559   \n",
       "8        752  [South Australia] has the most expensive elect...       783374   \n",
       "9        752  [South Australia] has the most expensive elect...       710661   \n",
       "10       752  [South Australia] has the most expensive elect...      1202442   \n",
       "11       752  [South Australia] has the most expensive elect...      1182965   \n",
       "\n",
       "                                             evidence  label  \n",
       "0   [citation needed] South Australia has the high...      1  \n",
       "1   \"South Australia has the highest power prices ...      1  \n",
       "2   Through the program, the organizations created...      0  \n",
       "3   This power is normally generated at power plan...      0  \n",
       "4   Though the country 's supply of electricity ne...      0  \n",
       "5   Although Mew Mew Power has not been released t...      0  \n",
       "6   Visitors from the USA, Brazil, Japan, China, A...      0  \n",
       "7   The first checkpoint was at Berri, South Austr...      0  \n",
       "8   A study done by Greenpeace International, the ...      0  \n",
       "9   Due to Lesotho's economic and geographical rel...      0  \n",
       "10  It is found in Australia, where it has been re...      0  \n",
       "11  The institute’s mission is to create a sociall...      0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test samples count: 197278\n",
      "Test samples overview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim_id</th>\n",
       "      <th>text</th>\n",
       "      <th>evidence_id</th>\n",
       "      <th>evidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2967</td>\n",
       "      <td>The contribution of waste heat to the global c...</td>\n",
       "      <td>1460</td>\n",
       "      <td>In the case of a four-stroke Otto cycle, techn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2967</td>\n",
       "      <td>The contribution of waste heat to the global c...</td>\n",
       "      <td>8304</td>\n",
       "      <td>As the Earth's climate warms, we are seeing ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2967</td>\n",
       "      <td>The contribution of waste heat to the global c...</td>\n",
       "      <td>11688</td>\n",
       "      <td>In late November 2016 surveys of 62 reefs show...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2967</td>\n",
       "      <td>The contribution of waste heat to the global c...</td>\n",
       "      <td>13436</td>\n",
       "      <td>Gruen made contributions in a broad range of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2967</td>\n",
       "      <td>The contribution of waste heat to the global c...</td>\n",
       "      <td>19067</td>\n",
       "      <td>However, during last two decades there has bee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>2967</td>\n",
       "      <td>The contribution of waste heat to the global c...</td>\n",
       "      <td>1183635</td>\n",
       "      <td>The study noted the influence of Michael Crich...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>2967</td>\n",
       "      <td>The contribution of waste heat to the global c...</td>\n",
       "      <td>1185839</td>\n",
       "      <td>It could prove to be the most inexorable, howe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>2967</td>\n",
       "      <td>The contribution of waste heat to the global c...</td>\n",
       "      <td>1198065</td>\n",
       "      <td>A report released in March 2012 by the Intergo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>2967</td>\n",
       "      <td>The contribution of waste heat to the global c...</td>\n",
       "      <td>1202564</td>\n",
       "      <td>In warmer climates no additional heat would be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>2967</td>\n",
       "      <td>The contribution of waste heat to the global c...</td>\n",
       "      <td>1203675</td>\n",
       "      <td>The book describes the consequences of uncheck...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     claim_id                                               text  evidence_id  \\\n",
       "0        2967  The contribution of waste heat to the global c...         1460   \n",
       "1        2967  The contribution of waste heat to the global c...         8304   \n",
       "2        2967  The contribution of waste heat to the global c...        11688   \n",
       "3        2967  The contribution of waste heat to the global c...        13436   \n",
       "4        2967  The contribution of waste heat to the global c...        19067   \n",
       "..        ...                                                ...          ...   \n",
       "173      2967  The contribution of waste heat to the global c...      1183635   \n",
       "174      2967  The contribution of waste heat to the global c...      1185839   \n",
       "175      2967  The contribution of waste heat to the global c...      1198065   \n",
       "176      2967  The contribution of waste heat to the global c...      1202564   \n",
       "177      2967  The contribution of waste heat to the global c...      1203675   \n",
       "\n",
       "                                              evidence  \n",
       "0    In the case of a four-stroke Otto cycle, techn...  \n",
       "1    As the Earth's climate warms, we are seeing ma...  \n",
       "2    In late November 2016 surveys of 62 reefs show...  \n",
       "3    Gruen made contributions in a broad range of t...  \n",
       "4    However, during last two decades there has bee...  \n",
       "..                                                 ...  \n",
       "173  The study noted the influence of Michael Crich...  \n",
       "174  It could prove to be the most inexorable, howe...  \n",
       "175  A report released in March 2012 by the Intergo...  \n",
       "176  In warmer climates no additional heat would be...  \n",
       "177  The book describes the consequences of uncheck...  \n",
       "\n",
       "[178 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Training positive samples count:\", df_tran_pos_count)\n",
    "print(\"Training negative samples count:\", df_tran_neg_count)\n",
    "print(\"Training samples overview:\")\n",
    "display(df_tran[df_tran['claim_id'] == df_tran['claim_id'].iloc[0]])\n",
    "\n",
    "print(\"Validation positive samples count:\", df_vald_pos_count)\n",
    "print(\"Validation negative samples count:\", df_vald_neg_count)\n",
    "print(\"Validation samples overview:\")\n",
    "display(df_vald[df_vald['claim_id'] == df_vald['claim_id'].iloc[0]])\n",
    "\n",
    "print(\"Test samples count:\", df_test.shape[0])\n",
    "print(\"Test samples overview:\")\n",
    "display(df_test[df_test['claim_id'] == df_test['claim_id'].iloc[0]] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4963ff9",
   "metadata": {},
   "source": [
    "## 1.3 Torch Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d99bea58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class Task1Dataset(Dataset):\n",
    "    def __init__(self, df, tokenizer=None):\n",
    "        self.samples = df.to_dict('records')\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        if self.tokenizer:\n",
    "            input_text = sample['text'] + \" [SEP] \" + sample['evidence']\n",
    "            encoded = self.tokenizer(input_text, truncation=True, padding='max_length', max_length=512, return_tensors='pt')\n",
    "            # Flatten the tensors since return_tensors='pt' gives 1-dim tensors\n",
    "            sample['input_ids'] = encoded['input_ids'].squeeze()\n",
    "            sample['attention_mask'] = encoded['attention_mask'].squeeze()\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4800d6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/python311/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('distilroberta-base')\n",
    "\n",
    "# Create Datasets\n",
    "dataset_tran = Task1Dataset(df_tran, tokenizer=tokenizer)\n",
    "dataset_vald = Task1Dataset(df_vald, tokenizer=tokenizer)\n",
    "dataset_test = Task1Dataset(df_test, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1e3eedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample keys: ['claim_id', 'text', 'evidence_id', 'evidence', 'label', 'input_ids', 'attention_mask']\n",
      "Input text length: 152\n",
      "Input IDs shape: torch.Size([512])\n",
      "Attention mask shape: torch.Size([512])\n",
      "Label: 1\n"
     ]
    }
   ],
   "source": [
    "# Test the Dataset\n",
    "sample_temp = dataset_tran[0]\n",
    "print(\"Sample keys:\", list(sample_temp.keys()))\n",
    "print(\"Input text length:\", len(sample_temp['text']))\n",
    "print(\"Input IDs shape:\", sample_temp['input_ids'].shape)\n",
    "print(\"Attention mask shape:\", sample_temp['attention_mask'].shape)\n",
    "print(\"Label:\", sample_temp['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "193daf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "dataloader_tran = DataLoader(dataset_tran, batch_size=16, shuffle=True)\n",
    "dataloader_vald = DataLoader(dataset_vald, batch_size=16, shuffle=False)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2f5f5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch keys: ['claim_id', 'text', 'evidence_id', 'evidence', 'label', 'input_ids', 'attention_mask']\n",
      "Batch input IDs shape: torch.Size([16, 512])\n",
      "Batch attention mask shape: torch.Size([16, 512])\n",
      "Batch labels shape: torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "# Test the Dataloader\n",
    "batch_temp = next(iter(dataloader_tran))\n",
    "print(\"Batch keys:\", list(batch_temp.keys()))\n",
    "print(\"Batch input IDs shape:\", batch_temp['input_ids'].shape)\n",
    "print(\"Batch attention mask shape:\", batch_temp['attention_mask'].shape)\n",
    "print(\"Batch labels shape:\", batch_temp['label'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89fe107",
   "metadata": {},
   "source": [
    "## 2. Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cad2dd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DistilRoBERTa Classifier Model\n",
    "class DistilRoBERTaClassifier(nn.Module):\n",
    "    def __init__(self, num_labels=2):\n",
    "        super(DistilRoBERTaClassifier, self).__init__()\n",
    "        self.distilroberta = DistilBertModel.from_pretrained('distilroberta-base')\n",
    "        self.classifier = nn.Linear(self.distilroberta.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.distilroberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0]  # Use the first token's representation\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86971476",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/python311/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are using a model of type roberta to instantiate a model of type distilbert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of DistilBertModel were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.word_embeddings.weight', 'transformer.layer.0.attention.k_lin.bias', 'transformer.layer.0.attention.k_lin.weight', 'transformer.layer.0.attention.out_lin.bias', 'transformer.layer.0.attention.out_lin.weight', 'transformer.layer.0.attention.q_lin.bias', 'transformer.layer.0.attention.q_lin.weight', 'transformer.layer.0.attention.v_lin.bias', 'transformer.layer.0.attention.v_lin.weight', 'transformer.layer.0.ffn.lin1.bias', 'transformer.layer.0.ffn.lin1.weight', 'transformer.layer.0.ffn.lin2.bias', 'transformer.layer.0.ffn.lin2.weight', 'transformer.layer.0.output_layer_norm.bias', 'transformer.layer.0.output_layer_norm.weight', 'transformer.layer.0.sa_layer_norm.bias', 'transformer.layer.0.sa_layer_norm.weight', 'transformer.layer.1.attention.k_lin.bias', 'transformer.layer.1.attention.k_lin.weight', 'transformer.layer.1.attention.out_lin.bias', 'transformer.layer.1.attention.out_lin.weight', 'transformer.layer.1.attention.q_lin.bias', 'transformer.layer.1.attention.q_lin.weight', 'transformer.layer.1.attention.v_lin.bias', 'transformer.layer.1.attention.v_lin.weight', 'transformer.layer.1.ffn.lin1.bias', 'transformer.layer.1.ffn.lin1.weight', 'transformer.layer.1.ffn.lin2.bias', 'transformer.layer.1.ffn.lin2.weight', 'transformer.layer.1.output_layer_norm.bias', 'transformer.layer.1.output_layer_norm.weight', 'transformer.layer.1.sa_layer_norm.bias', 'transformer.layer.1.sa_layer_norm.weight', 'transformer.layer.2.attention.k_lin.bias', 'transformer.layer.2.attention.k_lin.weight', 'transformer.layer.2.attention.out_lin.bias', 'transformer.layer.2.attention.out_lin.weight', 'transformer.layer.2.attention.q_lin.bias', 'transformer.layer.2.attention.q_lin.weight', 'transformer.layer.2.attention.v_lin.bias', 'transformer.layer.2.attention.v_lin.weight', 'transformer.layer.2.ffn.lin1.bias', 'transformer.layer.2.ffn.lin1.weight', 'transformer.layer.2.ffn.lin2.bias', 'transformer.layer.2.ffn.lin2.weight', 'transformer.layer.2.output_layer_norm.bias', 'transformer.layer.2.output_layer_norm.weight', 'transformer.layer.2.sa_layer_norm.bias', 'transformer.layer.2.sa_layer_norm.weight', 'transformer.layer.3.attention.k_lin.bias', 'transformer.layer.3.attention.k_lin.weight', 'transformer.layer.3.attention.out_lin.bias', 'transformer.layer.3.attention.out_lin.weight', 'transformer.layer.3.attention.q_lin.bias', 'transformer.layer.3.attention.q_lin.weight', 'transformer.layer.3.attention.v_lin.bias', 'transformer.layer.3.attention.v_lin.weight', 'transformer.layer.3.ffn.lin1.bias', 'transformer.layer.3.ffn.lin1.weight', 'transformer.layer.3.ffn.lin2.bias', 'transformer.layer.3.ffn.lin2.weight', 'transformer.layer.3.output_layer_norm.bias', 'transformer.layer.3.output_layer_norm.weight', 'transformer.layer.3.sa_layer_norm.bias', 'transformer.layer.3.sa_layer_norm.weight', 'transformer.layer.4.attention.k_lin.bias', 'transformer.layer.4.attention.k_lin.weight', 'transformer.layer.4.attention.out_lin.bias', 'transformer.layer.4.attention.out_lin.weight', 'transformer.layer.4.attention.q_lin.bias', 'transformer.layer.4.attention.q_lin.weight', 'transformer.layer.4.attention.v_lin.bias', 'transformer.layer.4.attention.v_lin.weight', 'transformer.layer.4.ffn.lin1.bias', 'transformer.layer.4.ffn.lin1.weight', 'transformer.layer.4.ffn.lin2.bias', 'transformer.layer.4.ffn.lin2.weight', 'transformer.layer.4.output_layer_norm.bias', 'transformer.layer.4.output_layer_norm.weight', 'transformer.layer.4.sa_layer_norm.bias', 'transformer.layer.4.sa_layer_norm.weight', 'transformer.layer.5.attention.k_lin.bias', 'transformer.layer.5.attention.k_lin.weight', 'transformer.layer.5.attention.out_lin.bias', 'transformer.layer.5.attention.out_lin.weight', 'transformer.layer.5.attention.q_lin.bias', 'transformer.layer.5.attention.q_lin.weight', 'transformer.layer.5.attention.v_lin.bias', 'transformer.layer.5.attention.v_lin.weight', 'transformer.layer.5.ffn.lin1.bias', 'transformer.layer.5.ffn.lin1.weight', 'transformer.layer.5.ffn.lin2.bias', 'transformer.layer.5.ffn.lin2.weight', 'transformer.layer.5.output_layer_norm.bias', 'transformer.layer.5.output_layer_norm.weight', 'transformer.layer.5.sa_layer_norm.bias', 'transformer.layer.5.sa_layer_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output shape: torch.Size([16, 2])\n"
     ]
    }
   ],
   "source": [
    "# Test the Model\n",
    "model = DistilRoBERTaClassifier(num_labels=2)\n",
    "outputs_temp = model(batch_temp['input_ids'], batch_temp['attention_mask'])\n",
    "print(\"Model output shape:\", outputs_temp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc13574a",
   "metadata": {},
   "source": [
    "## 3. Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06c131ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=5, learning_rate=2e-5, device='cpu'):\n",
    "\n",
    "    model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        # Iterate over training batches\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        val_accuracy = evaluate_model(model, val_loader, device)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4bd5e824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, device='cpu'):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc7806ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# 设置设备\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea06c7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/python311/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You are using a model of type roberta to instantiate a model of type distilbert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of DistilBertModel were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['embeddings.LayerNorm.bias', 'embeddings.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'embeddings.word_embeddings.weight', 'transformer.layer.0.attention.k_lin.bias', 'transformer.layer.0.attention.k_lin.weight', 'transformer.layer.0.attention.out_lin.bias', 'transformer.layer.0.attention.out_lin.weight', 'transformer.layer.0.attention.q_lin.bias', 'transformer.layer.0.attention.q_lin.weight', 'transformer.layer.0.attention.v_lin.bias', 'transformer.layer.0.attention.v_lin.weight', 'transformer.layer.0.ffn.lin1.bias', 'transformer.layer.0.ffn.lin1.weight', 'transformer.layer.0.ffn.lin2.bias', 'transformer.layer.0.ffn.lin2.weight', 'transformer.layer.0.output_layer_norm.bias', 'transformer.layer.0.output_layer_norm.weight', 'transformer.layer.0.sa_layer_norm.bias', 'transformer.layer.0.sa_layer_norm.weight', 'transformer.layer.1.attention.k_lin.bias', 'transformer.layer.1.attention.k_lin.weight', 'transformer.layer.1.attention.out_lin.bias', 'transformer.layer.1.attention.out_lin.weight', 'transformer.layer.1.attention.q_lin.bias', 'transformer.layer.1.attention.q_lin.weight', 'transformer.layer.1.attention.v_lin.bias', 'transformer.layer.1.attention.v_lin.weight', 'transformer.layer.1.ffn.lin1.bias', 'transformer.layer.1.ffn.lin1.weight', 'transformer.layer.1.ffn.lin2.bias', 'transformer.layer.1.ffn.lin2.weight', 'transformer.layer.1.output_layer_norm.bias', 'transformer.layer.1.output_layer_norm.weight', 'transformer.layer.1.sa_layer_norm.bias', 'transformer.layer.1.sa_layer_norm.weight', 'transformer.layer.2.attention.k_lin.bias', 'transformer.layer.2.attention.k_lin.weight', 'transformer.layer.2.attention.out_lin.bias', 'transformer.layer.2.attention.out_lin.weight', 'transformer.layer.2.attention.q_lin.bias', 'transformer.layer.2.attention.q_lin.weight', 'transformer.layer.2.attention.v_lin.bias', 'transformer.layer.2.attention.v_lin.weight', 'transformer.layer.2.ffn.lin1.bias', 'transformer.layer.2.ffn.lin1.weight', 'transformer.layer.2.ffn.lin2.bias', 'transformer.layer.2.ffn.lin2.weight', 'transformer.layer.2.output_layer_norm.bias', 'transformer.layer.2.output_layer_norm.weight', 'transformer.layer.2.sa_layer_norm.bias', 'transformer.layer.2.sa_layer_norm.weight', 'transformer.layer.3.attention.k_lin.bias', 'transformer.layer.3.attention.k_lin.weight', 'transformer.layer.3.attention.out_lin.bias', 'transformer.layer.3.attention.out_lin.weight', 'transformer.layer.3.attention.q_lin.bias', 'transformer.layer.3.attention.q_lin.weight', 'transformer.layer.3.attention.v_lin.bias', 'transformer.layer.3.attention.v_lin.weight', 'transformer.layer.3.ffn.lin1.bias', 'transformer.layer.3.ffn.lin1.weight', 'transformer.layer.3.ffn.lin2.bias', 'transformer.layer.3.ffn.lin2.weight', 'transformer.layer.3.output_layer_norm.bias', 'transformer.layer.3.output_layer_norm.weight', 'transformer.layer.3.sa_layer_norm.bias', 'transformer.layer.3.sa_layer_norm.weight', 'transformer.layer.4.attention.k_lin.bias', 'transformer.layer.4.attention.k_lin.weight', 'transformer.layer.4.attention.out_lin.bias', 'transformer.layer.4.attention.out_lin.weight', 'transformer.layer.4.attention.q_lin.bias', 'transformer.layer.4.attention.q_lin.weight', 'transformer.layer.4.attention.v_lin.bias', 'transformer.layer.4.attention.v_lin.weight', 'transformer.layer.4.ffn.lin1.bias', 'transformer.layer.4.ffn.lin1.weight', 'transformer.layer.4.ffn.lin2.bias', 'transformer.layer.4.ffn.lin2.weight', 'transformer.layer.4.output_layer_norm.bias', 'transformer.layer.4.output_layer_norm.weight', 'transformer.layer.4.sa_layer_norm.bias', 'transformer.layer.4.sa_layer_norm.weight', 'transformer.layer.5.attention.k_lin.bias', 'transformer.layer.5.attention.k_lin.weight', 'transformer.layer.5.attention.out_lin.bias', 'transformer.layer.5.attention.out_lin.weight', 'transformer.layer.5.attention.q_lin.bias', 'transformer.layer.5.attention.q_lin.weight', 'transformer.layer.5.attention.v_lin.bias', 'transformer.layer.5.attention.v_lin.weight', 'transformer.layer.5.ffn.lin1.bias', 'transformer.layer.5.ffn.lin1.weight', 'transformer.layer.5.ffn.lin2.bias', 'transformer.layer.5.ffn.lin2.weight', 'transformer.layer.5.output_layer_norm.bias', 'transformer.layer.5.output_layer_norm.weight', 'transformer.layer.5.sa_layer_norm.bias', 'transformer.layer.5.sa_layer_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 训练模型\u001b[39;00m\n\u001b[32m      2\u001b[39m model = DistilRoBERTaClassifier(num_labels=\u001b[32m2\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m model = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_tran\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_vald\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, num_epochs, learning_rate, device)\u001b[39m\n\u001b[32m     20\u001b[39m outputs = model(input_ids, attention_mask)\n\u001b[32m     21\u001b[39m loss = criterion(outputs, labels)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m optimizer.step()\n\u001b[32m     25\u001b[39m total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/python311/lib/python3.11/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/python311/lib/python3.11/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/python311/lib/python3.11/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "model = DistilRoBERTaClassifier(num_labels=2)\n",
    "model = train_model(model, dataloader_tran, dataloader_vald, num_epochs=3, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
