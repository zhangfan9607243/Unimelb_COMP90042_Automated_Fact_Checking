{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf66d71-b38b-4343-a28d-cda5068142a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import nltk\n",
    "from unidecode import unidecode\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2708cd-37ee-486e-907c-866ab9f323f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "\n",
    "    def __init__(self, evdn_file, tran_file, deva_file, devb_file, test_file):\n",
    "        \n",
    "        print(\"Loading data started...\")\n",
    "        \n",
    "        self.corpus_t1 = set()\n",
    "        self.sentence_t1 = []\n",
    "        self.corpus_t2 = set()\n",
    "        self.sentence_t2 = []\n",
    "        \n",
    "        self.evdn_origin = self.read(evdn_file)\n",
    "        self.tran_origin = self.read(tran_file)\n",
    "        self.deva_origin = self.read(deva_file)\n",
    "        self.test_origin = self.read(test_file)\n",
    "        \n",
    "        self.retain_evidence_list = set(self.retain_evidence(self.tran_origin, self.deva_origin))\n",
    "\n",
    "        print(\"   1/10 Loading evidence (part) for t1 started...\")\n",
    "        self.evdn_df_part_t1 = self.preprocess_evdn_t1_part(data=self.evdn_origin)\n",
    "        print(\"        Loading evidence (part) for t1 finished.\")\n",
    "        print(\"   2/10 Loading evidence (full) for t1 started...\")\n",
    "        self.evdn_df_full_t1 = self.preprocess_evdn_t1_full(data=self.evdn_origin)\n",
    "        print(\"        Loading evidence (full) for t1 finished.\")\n",
    "        print(\"   3/10 Loading training data for t1 started...\")\n",
    "        self.tran_df_t1 = self.preprocess_data_t1(data=self.tran_origin, test=False)\n",
    "        print(\"        Loading training data for t1 finished.\")\n",
    "        print(\"   4/10 Loading devalopment data for t1 started...\")\n",
    "        self.deva_df_t1 = self.preprocess_data_t1(data=self.deva_origin, test=False)\n",
    "        print(\"        Loading devalopment data for t1 finished.\")\n",
    "        print(\"   5/10 Loading testing data for t1 started...\")\n",
    "        self.test_df_t1 = self.preprocess_data_t1(data=self.test_origin, test=True)\n",
    "        print(\"        Loading testing data for t1 finished.\")\n",
    "        \n",
    "        print(\"   6/10 Loading evidence (part) for t2 started...\")\n",
    "        self.evdn_df_part_t2 = self.preprocess_evdn_t2_part(data=self.evdn_origin)\n",
    "        print(\"        Loading evidence (part) for t2 finished.\")\n",
    "        print(\"   7/10 Loading evidence (full) for t2 started...\")\n",
    "        self.evdn_df_part_t2 = self.preprocess_evdn_t2_full(data=self.evdn_origin)\n",
    "        print(\"        Loading evidence (full) for t2 finished.\")\n",
    "        print(\"   8/10 Loading training data for t2 started...\")\n",
    "        self.tran_df_t2 = self.preprocess_data_t2(data=self.tran_origin, test=False)\n",
    "        print(\"        Loading training data for t2 finished.\")\n",
    "        print(\"   9/10 Loading devalopment data for t2 started...\")\n",
    "        self.deva_df_t2 = self.preprocess_data_t2(data=self.deva_origin, test=False)\n",
    "        print(\"        Loading devalopment data for t2 finished.\")\n",
    "        print(\"  10/10 Loading testing data for t2 started...\")\n",
    "        self.test_df_t2 = self.preprocess_data_t2(data=self.test_origin, test=True)\n",
    "        print(\"        Loading testing data for t2 finished.\")\n",
    "        print()\n",
    "        \n",
    "        self.summary_statistics = self.show_statistics()\n",
    "        \n",
    "    def read(self, file_name):\n",
    "        with open(file_name, 'r', encoding='utf8') as data:\n",
    "            json_data = json.load(data)\n",
    "        return json_data\n",
    "    \n",
    "    def write(self, y_pred_final, evidence_index_list):\n",
    "        final_dict = self.test_origin\n",
    "        index_list = list(self.test_dataframe[\"index\"])\n",
    "        for i in range(len(index_list)):\n",
    "            index = index_list[i]\n",
    "            evidence = []\n",
    "            for evidence_index in evidence_index_list[i]:\n",
    "                evidence.append(\"evidence-\"+str(evidence_index))\n",
    "            label = y_pred_final[i]\n",
    "            final_dict[\"claim-\"+str(index)][\"claim_label\"] = label\n",
    "            final_dict[\"claim-\"+str(index)][\"evidences\"] = evidence\n",
    "        json_str = json.dumps(final_dict)\n",
    "        with open('test-claims-predictions.json', 'w') as json_file:\n",
    "            json_file.write(json_str)\n",
    "    \n",
    "    def show_statistics(self):\n",
    "        print(\"Loading data finished:\")\n",
    "        \n",
    "        evdn_len_full = []\n",
    "        for sentence in self.evdn_df_full_t1[\"evidence\"]:\n",
    "            evdn_len_full.append(len(nltk.tokenize.word_tokenize(sentence)))\n",
    "        evdn_len_part = []\n",
    "        for sentence in self.evdn_df_part_t1[\"evidence\"]:\n",
    "            evdn_len_part.append(len(nltk.tokenize.word_tokenize(sentence)))\n",
    "            \n",
    "        evdn_text_leng_list = evdn_len_full + evdn_len_part\n",
    "        \n",
    "        tran_evdn_leng_list = []\n",
    "        deva_evdn_leng_list = []\n",
    "        for key, value in self.tran_origin.items():\n",
    "            count = 0\n",
    "            for evdn in value[\"evidences\"]:\n",
    "                count += 1\n",
    "            tran_evdn_leng_list.append(count)\n",
    "        for key, value in self.deva_origin.items():\n",
    "            count = 0\n",
    "            for evdn in value[\"evidences\"]:\n",
    "                count += 1\n",
    "            deva_evdn_leng_list.append(count)\n",
    "        \n",
    "        data_evdn_leng_list = tran_evdn_leng_list + deva_evdn_leng_list\n",
    "        \n",
    "        tran_token_leng_list = []\n",
    "        deva_token_leng_list = []\n",
    "        test_token_leng_list = []\n",
    "        for key, value in self.tran_origin.items():\n",
    "            tran_token_leng_list.append(len(nltk.tokenize.word_tokenize(value[\"claim_text\"])))\n",
    "        for key, value in self.deva_origin.items():\n",
    "            deva_token_leng_list.append(len(nltk.tokenize.word_tokenize(value[\"claim_text\"])))\n",
    "        for key, value in self.test_origin.items():\n",
    "            test_token_leng_list.append(len(nltk.tokenize.word_tokenize(value[\"claim_text\"])))\n",
    "        \n",
    "        data_text_leng_list = tran_token_leng_list + deva_token_leng_list + test_token_leng_list\n",
    "        \n",
    "        print(\"Data Summary:\")\n",
    "        print(\"   1. Evidence\")\n",
    "        print(\"      (1) Full set of evidence\")\n",
    "        print(\"          Original number of evidence = \" + str(len(self.evdn_df_full_t1)) + \".\")\n",
    "        print(\"          Maximum number of tokens in evidence text = \" + str(round(np.max(evdn_len_full),0)) + \".\")\n",
    "        print(\"          Minimum number of tokens in evidence text = \" + str(round(np.min(evdn_len_full),0)) + \".\")\n",
    "        print(\"          Average number of tokens in evidence text = \" + str(round(np.mean(evdn_len_full),2)) + \".\")\n",
    "        print(\"      (2) Retained set of evidence\")\n",
    "        print(\"          Retained number of evidence = \" + str(len(self.evdn_df_part_t1)) + \".\")\n",
    "        print(\"          Maximum number of tokens in text = \" + str(round(np.max(evdn_len_part),0)) + \".\")\n",
    "        print(\"          Minimum number of tokens in text = \" + str(round(np.min(evdn_len_part),0)) + \".\")\n",
    "        print(\"          Average number of tokens in text = \" + str(round(np.mean(evdn_len_part),2)) + \".\")\n",
    "        print(\"   2. Data Sets\")\n",
    "        print(\"      (1) Training Data\")\n",
    "        print(\"          Sample size of training data = \" + str(len(self.tran_origin)) + \".\")\n",
    "        print(\"          Maximum number of tokens in claim text = \" + str(round(np.max(tran_token_leng_list),0)) + \".\")\n",
    "        print(\"          Minimum number of tokens in claim text = \" + str(round(np.min(tran_token_leng_list),0)) + \".\")\n",
    "        print(\"          Average number of tokens in claim text = \" + str(round(np.mean(tran_token_leng_list),2)) + \".\")\n",
    "        print(\"          Maximum number of evidence for training data = \" + str(round(np.max(tran_evdn_leng_list),0)) + \".\")\n",
    "        print(\"          Minimum number of evidence for training data = \" + str(round(np.min(tran_evdn_leng_list),0)) + \".\")\n",
    "        print(\"          Average number of evidence for training data = \" + str(round(np.mean(tran_evdn_leng_list),2)) + \".\")\n",
    "        print(\"      (2) Developing Data\")\n",
    "        print(\"          Sample size of developing data = \" + str(len(self.deva_origin)) + \".\")\n",
    "        print(\"          Maximum number of tokens in claim text = \" + str(round(np.max(deva_token_leng_list),0)) + \".\")\n",
    "        print(\"          Minimum number of tokens in claim text = \" + str(round(np.min(deva_token_leng_list),0)) + \".\")\n",
    "        print(\"          Average number of tokens in claim text = \" + str(round(np.mean(deva_token_leng_list),2)) + \".\")\n",
    "        print(\"          Maximum number of evidence for developing data = \" + str(round(np.max(deva_evdn_leng_list),0)) + \".\")\n",
    "        print(\"          Minimum number of evidence for developing data = \" + str(round(np.min(deva_evdn_leng_list),0)) + \".\")\n",
    "        print(\"          Average number of evidence for developing data = \" + str(round(np.mean(deva_evdn_leng_list),2)) + \".\")\n",
    "        print(\"      (3) Testing Data\")\n",
    "        print(\"          Sample size of testing test = \" + str(len(self.test_origin)) + \".\")\n",
    "        print(\"          Maximum number of tokens in claim text = \" + str(round(np.max(test_token_leng_list),0)) + \".\")\n",
    "        print(\"          Minimum number of tokens in claim text = \" + str(round(np.min(test_token_leng_list),0)) + \".\")\n",
    "        print(\"          Average number of tokens in claim text = \" + str(round(np.mean(test_token_leng_list),2)) + \".\")\n",
    "        \n",
    "        return evdn_text_leng_list, data_evdn_leng_list, data_text_leng_list\n",
    "        \n",
    "    def retain_evidence(self,tran,deva):\n",
    "        retain_evidence_list = []\n",
    "        for key, value in tran.items():\n",
    "            for evdn in value[\"evidences\"]:\n",
    "                retain_evidence_list.append(int(evdn.split(\"-\")[1]))\n",
    "        for key, value in deva.items():\n",
    "            for evdn in value[\"evidences\"]:\n",
    "                retain_evidence_list.append(int(evdn.split(\"-\")[1]))\n",
    "        return retain_evidence_list\n",
    "        \n",
    "    def process_sentence_t1(self, sentence):\n",
    "        stop_words = set(nltk.corpus.stopwords.words('english')) | set(list(string.punctuation))\n",
    "        lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        stemmer = nltk.stem.PorterStemmer()\n",
    "        valid_list = \" 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "        \n",
    "        if isinstance(sentence, str):\n",
    "            sentence_new = \"\"\n",
    "            for char in sentence:\n",
    "                if char in valid_list:\n",
    "                    sentence_new += char\n",
    "            words = []\n",
    "            for word in [i for i in nltk.tokenize.word_tokenize(sentence_new.lower()) if i not in stop_words]:\n",
    "                words.append(stemmer.stem(lemmatizer.lemmatize(word)))\n",
    "            sentence_final = unidecode(\" \".join(words)).strip()\n",
    "            if sentence_final == \"\":\n",
    "                return \"nan nan\"\n",
    "            else:\n",
    "                return sentence_final\n",
    "        else:\n",
    "            return \"nan nan\"\n",
    "    \n",
    "    def process_sentence_t2(self, sentence):\n",
    "        lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "        valid_list = \" 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "        if isinstance(sentence, str):\n",
    "            sentence_new = \"\"\n",
    "            for char in sentence:\n",
    "                if char in valid_list:\n",
    "                    sentence_new += char\n",
    "            words = []\n",
    "            for word in [i for i in nltk.tokenize.word_tokenize(sentence_new.lower())]:\n",
    "                words.append(lemmatizer.lemmatize(word))\n",
    "            sentence_final = unidecode(\" \".join(words)).strip()\n",
    "            if sentence_final == \"\":\n",
    "                return \"nan nan\"\n",
    "            else:\n",
    "                return sentence_final\n",
    "        else:\n",
    "            return \"nan nan\"\n",
    "    \n",
    "    def preprocess_data_t1(self, data, test):\n",
    "        preprocessed_data = []\n",
    "        for key, value in data.items():   \n",
    "            claim_index = int(key.split(\"-\")[1])\n",
    "            \n",
    "            claim_text = self.process_sentence_t1(value[\"claim_text\"])\n",
    "            self.sentence_t1.append(claim_text)\n",
    "            for word in nltk.tokenize.word_tokenize(claim_text):\n",
    "                self.corpus_t1.add(word)\n",
    "            \n",
    "            if test == False:\n",
    "                label = value[\"claim_label\"]\n",
    "                evidence_index_string = \"\"\n",
    "                evidence_index_list = []\n",
    "                for evdn in value[\"evidences\"]:\n",
    "                    index = int(evdn.split(\"-\")[1])\n",
    "                    evidence_index_string += str(index) + \" \"\n",
    "                    evidence_index_list.append(index)\n",
    "                evidence_string = \"\"\n",
    "                for i in evidence_index_list:\n",
    "                    evidence_string += self.evdn_df_part_t1[\"evidence\"][i] + \" \"\n",
    "                preprocessed_data.append([claim_index, claim_text, evidence_index_string, evidence_string, label])\n",
    "            else:\n",
    "                preprocessed_data.append([claim_index, claim_text])\n",
    "            \n",
    "        if test == False:\n",
    "            preprocessed_data_df = pd.DataFrame(preprocessed_data)\n",
    "            preprocessed_data_df.columns = [\"claim_index\", \"claim\", \"evidence_index\", \"evidence_text\", \"label\"]\n",
    "        else:\n",
    "            preprocessed_data_df = pd.DataFrame(preprocessed_data)\n",
    "            preprocessed_data_df.columns = [\"claim_index\", \"claim\"]\n",
    "        \n",
    "        return preprocessed_data_df\n",
    "    \n",
    "    def preprocess_data_t2(self, data, test):\n",
    "        preprocessed_data = []\n",
    "        for key, value in data.items():\n",
    "            claim_index = int(key.split(\"-\")[1])\n",
    "\n",
    "            claim_text = self.process_sentence_t2(value[\"claim_text\"])\n",
    "            self.sentence_t2.append(claim_text)\n",
    "\n",
    "            for word in nltk.tokenize.word_tokenize(claim_text):\n",
    "                self.corpus_t2.add(word)\n",
    "    \n",
    "            if test == False:\n",
    "                label = value[\"claim_label\"]\n",
    "                evidence_index_string = \"\"\n",
    "                evidence_index_list = []\n",
    "                for evdn in value[\"evidences\"]:\n",
    "                    index = int(evdn.split(\"-\")[1])\n",
    "                    evidence_index_string += str(index) + \" \"\n",
    "                    evidence_index_list.append(index)\n",
    "                evidence_string = \"\"\n",
    "                for i in evidence_index_list:\n",
    "                    evidence_string += self.evdn_df_part_t1[\"evidence\"][i] + \" \"\n",
    "                preprocessed_data.append([claim_index, claim_text, evidence_index_string, evidence_string, label])\n",
    "            else:\n",
    "                preprocessed_data.append([claim_index, claim_text])\n",
    "        if test == False:\n",
    "            preprocessed_data_df = pd.DataFrame(preprocessed_data)\n",
    "            preprocessed_data_df.columns = [\"claim_index\", \"claim\", \"evidence_index\", \"evidence_text\", \"label\"]\n",
    "        else:\n",
    "            preprocessed_data_df = pd.DataFrame(preprocessed_data)\n",
    "            preprocessed_data_df.columns = [\"claim_index\", \"claim\"]\n",
    "        \n",
    "        return preprocessed_data_df\n",
    "    \n",
    "    def preprocess_evdn_t1_full(self, data):\n",
    "        preprocessed_evdn = []\n",
    "        for key, value in data.items():\n",
    "            evdn_index = int(key.split(\"-\")[1]) \n",
    "            evdn_text = self.process_sentence_t1(value)\n",
    "            self.sentence_t1.append(evdn_text)\n",
    "            for word in evdn_text.split():\n",
    "                self.corpus_t1.add(word.lower())\n",
    "            preprocessed_evdn.append([evdn_index, evdn_text])\n",
    "        preprocessed_evdn_df = pd.DataFrame(preprocessed_evdn)\n",
    "        preprocessed_evdn_df.columns = [\"evdn_index\", \"evidence\"]\n",
    "        preprocessed_evdn_df = preprocessed_evdn_df.set_index(\"evdn_index\")\n",
    "        return preprocessed_evdn_df\n",
    "    \n",
    "    def preprocess_evdn_t2_full(self, data):\n",
    "        preprocessed_evdn = []\n",
    "        for key, value in data.items():\n",
    "            evdn_index = int(key.split(\"-\")[1])\n",
    "            evdn_text = self.process_sentence_t1(value)\n",
    "            self.sentence_t2.append(evdn_text)\n",
    "            for word in evdn_text.split():\n",
    "                self.corpus_t2.add(word.lower())\n",
    "            preprocessed_evdn.append([evdn_index, evdn_text])    \n",
    "        preprocessed_evdn_df = pd.DataFrame(preprocessed_evdn)\n",
    "        preprocessed_evdn_df.columns = [\"evdn_index\", \"evidence\"]\n",
    "        preprocessed_evdn_df = preprocessed_evdn_df.set_index(\"evdn_index\")\n",
    "        return preprocessed_evdn_df\n",
    "    \n",
    "    def preprocess_evdn_t1_part(self, data):\n",
    "        preprocessed_evdn = []\n",
    "        for key, value in data.items():\n",
    "            evdn_index = int(key.split(\"-\")[1])\n",
    "            if evdn_index in self.retain_evidence_list:\n",
    "                evdn_text = self.process_sentence_t1(value)\n",
    "                self.sentence_t1.append(evdn_text)\n",
    "                for word in evdn_text.split():\n",
    "                    self.corpus_t1.add(word.lower())\n",
    "                preprocessed_evdn.append([evdn_index, evdn_text])\n",
    "        preprocessed_evdn_df = pd.DataFrame(preprocessed_evdn)\n",
    "        preprocessed_evdn_df.columns = [\"evdn_index\", \"evidence\"]\n",
    "        preprocessed_evdn_df = preprocessed_evdn_df.set_index(\"evdn_index\")\n",
    "        return preprocessed_evdn_df\n",
    "    \n",
    "    def preprocess_evdn_t2_part(self, data):\n",
    "        preprocessed_evdn = []\n",
    "        for key, value in data.items():\n",
    "            evdn_index = int(key.split(\"-\")[1])\n",
    "            if evdn_index in self.retain_evidence_list:\n",
    "                evdn_text = self.process_sentence_t1(value)\n",
    "                self.sentence_t2.append(evdn_text)\n",
    "                for word in evdn_text.split():\n",
    "                    self.corpus_t2.add(word.lower())\n",
    "                preprocessed_evdn.append([evdn_index, evdn_text])    \n",
    "        preprocessed_evdn_df = pd.DataFrame(preprocessed_evdn)\n",
    "        preprocessed_evdn_df.columns = [\"evdn_index\", \"evidence\"]\n",
    "        preprocessed_evdn_df = preprocessed_evdn_df.set_index(\"evdn_index\")\n",
    "        return preprocessed_evdn_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fc7fc5-796e-419b-b6c6-fa17282e5b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data processor\n",
    "data_processor = DataProcessor(evdn_file=\"data_raw/evidence.json\", \n",
    "                               tran_file=\"data_raw/train-claims.json\", \n",
    "                               deva_file=\"data_raw/dev-claims.json\",\n",
    "                               devb_file=\"data_raw/dev-claims-baseline.json\",\n",
    "                               test_file=\"data_raw/test-claims-unlabelled.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17056df-3c82-4c9e-ad9e-8154c83db9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor.evdn_df_full_t1.to_csv(\"data_processed/evdn_full_df_t1.csv\")\n",
    "data_processor.evdn_df_part_t1.to_csv(\"data_processed/evdn_part_df_t1.csv\")\n",
    "data_processor.tran_df_t1.to_csv(\"data_processed/tran_df_t1.csv\")\n",
    "data_processor.deva_df_t1.to_csv(\"data_processed/deva_df_t1.csv\")\n",
    "data_processor.test_df_t1.to_csv(\"data_processed/test_df_t1.csv\")\n",
    "\n",
    "data_processor.evdn_df_full_t1.to_csv(\"data_processed/evdn_full_df_t2.csv\")\n",
    "data_processor.evdn_df_part_t1.to_csv(\"data_processed/evdn_part_df_t2.csv\")\n",
    "data_processor.tran_df_t2.to_csv(\"data_processed/tran_df_t2.csv\")\n",
    "data_processor.deva_df_t2.to_csv(\"data_processed/deva_df_t2.csv\")\n",
    "data_processor.test_df_t2.to_csv(\"data_processed/test_df_t2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4eec65-cfd0-43be-8d83-0de45c08ab13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
